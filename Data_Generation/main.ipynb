{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the data\n",
    "1. Generate the Heterogeneous graph\n",
    "2. Generate the feature set from the clinical notes.\n",
    "3. Generate the Labels\n",
    "4. Generate the k-metapath-based similarity matrices\n",
    "5. Convert the As to edge-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class heterogeneous_Graph:\n",
    "    def __init__(self, G):\n",
    "        self.G = G\n",
    "        Nodes = list(self.G.nodes())\n",
    "        self.Patients =    [v for v in Nodes if v[0]=='C']\n",
    "        self.Visits =      [v for v in Nodes if v[0]=='V']\n",
    "        self.Medications = [v for v in Nodes if v[0]=='M']\n",
    "        self.Diagnoses  =  [v for v in Nodes if v[0]=='D']\n",
    "        self.Procedures =  [v for v in Nodes if v[0]=='P']\n",
    "        self.Labs       =  [v for v in Nodes if v[0]=='L']\n",
    "        self.MicroBio   =  [v for v in Nodes if v[0]=='B']\n",
    "        self.Nodes = self.Patients  + self.Visits + self.Medications + self.Diagnoses + self.Procedures + self.Labs + self.MicroBio\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 DMPLB PC 500 True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "\n",
    "def save_list_as_pickle(L, given_path, file_name):\n",
    "    import pickle\n",
    "    print(f'saving to {given_path}/{file_name}.pkl')\n",
    "    with open(f'{given_path}/{file_name}.pkl', 'wb') as file:\n",
    "        pickle.dump(L, file)\n",
    "\n",
    "\n",
    "num_Diseases    = int(os.getenv('NUM_DISEASES', 203))  \n",
    "DISEASE_FILE    = os.getenv('DISEASE_FILE', f'DMPLB')  \n",
    "similarity_type = os.getenv('similarity_type', 'PC')  # options are PC: PathCount, SPS: Symmetric PathSim\n",
    "\n",
    "num_Sample      = int(os.getenv('num_Sample', 500))  \n",
    "r_u_sampling    = os.getenv('r_u_sampling', 'True')  \n",
    "SNF_ing         = os.getenv('SNF_ing', 'True')  \n",
    "\n",
    "\n",
    "if r_u_sampling=='True':\n",
    "    sampling = True\n",
    "else:\n",
    "    sampling = False\n",
    "\n",
    "if SNF_ing=='True':\n",
    "    SNF_ing = True\n",
    "else:\n",
    "    SNF_ing = False\n",
    "\n",
    "disease_data_path = '/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data'\n",
    "\n",
    "print(num_Diseases, DISEASE_FILE, similarity_type, num_Sample, sampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataframes...\n",
      "Splitting lab tests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data_Generation/module1/generating_HG.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lab_df.loc[:, 'ITEMID_FLAG'] = lab_df['ITEMID'].astype(str) + '_' + lab_df['FLAG'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of visits here is 58151\n",
      "Use the patients inside the new DataFrame....\n",
      "Dropping NaN visits\n",
      "General Information:\n",
      "---------------------------\n",
      "Number of Patients = 46517\n",
      "Number of Visits = 58929\n",
      "Number of Diagnosis = 203\n",
      "Number of procedures = 89\n",
      "Number of Medication = 592\n",
      "Number of Lab tests  = 993\n",
      "Number of MicroBio   = 92\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Extracting bipartite networks...\n",
      "\n",
      "Extracting and adding data of Visits\n",
      "\n",
      "Extracting and adding data of Diagnosis\n",
      "\n",
      "Extracting and adding data of Procedures\n",
      "\n",
      "Extracting and adding data of Medications\n",
      "\n",
      "Extracting and adding data of Lab tests\n",
      "\n",
      "Extracting and adding data of MicroBiology tests\n",
      "number of patients = 46437\n",
      "number of visits = 58929\n",
      "number of Medication = 592\n",
      "number of Diagnoses = 203\n",
      "number of Procedures = 89\n",
      "number of Labs = 993\n",
      "number of MicoBio = 92\n",
      "number of Edges = 5479842\n",
      "------------------------------------------\n",
      "\n",
      "Removing isolated nodes\n",
      "Number of PATIENTS to remove: 0\n",
      "Number of nodes to remove: 0\n",
      "number of patients = 46437\n",
      "number of visits = 58929\n",
      "number of Medication = 592\n",
      "number of Diagnoses = 203\n",
      "number of Procedures = 89\n",
      "number of Labs = 480\n",
      "number of MicoBio = 92\n",
      "number of Edges = 5456117\n",
      "------------------------------------------\n",
      "\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "number of patients = 46437\n",
      "number of visits = 58929\n",
      "number of Medication = 592\n",
      "number of Diagnoses = 203\n",
      "number of Procedures = 89\n",
      "number of Labs = 480\n",
      "number of MicoBio = 92\n",
      "number of Edges = 5456117\n",
      "------------------------------------------\n",
      "\n",
      "45937 500 46437\n",
      "Number of PATIENTS to remove: 45937\n",
      "Number of nodes to remove: 104134\n",
      "getting the feature set for all nodes\n",
      "getting the feature set for all nodes: visit_level\n",
      "getting the feature set for all nodes\n",
      "getting the feature set for all nodes: visit_level\n",
      "extracting As from HG\n",
      "\n",
      "[(2688, 2688), (2688, 2688), (2688, 2688), (2688, 2688), (2688, 2688), (2688, 2688)]\n",
      "=============================================================\n",
      "Multiplication phase...\n",
      "\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "Patient-Patient completed!\n",
      "\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n",
      "Done multiplication...\n",
      "multiplying (2688, 2688) * (2688, 2688) in parallel...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data_Generation/main.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data_Generation/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m YV \u001b[39m=\u001b[39m XY_inst\u001b[39m.\u001b[39mY_visit\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data_Generation/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# ======================= Computing the Meta Path based Similarities ======================\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data_Generation/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m MP_inst \u001b[39m=\u001b[39m MP\u001b[39m.\u001b[39;49mMeta_path(HG, similarity_type \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mPC\u001b[39;49m\u001b[39m'\u001b[39;49m, saving_path \u001b[39m=\u001b[39;49m saving_path)\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data_Generation/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# ==================================== SAVING =============================================\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://athena3.hprc.vcu.edu/lustre/home/almusawiaf/PhD_Projects/HGNN_Project2/Data_Generation/main.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m nx\u001b[39m.\u001b[39mwrite_gml(HG, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msaving_path\u001b[39m}\u001b[39;00m\u001b[39m/HG.gml\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/PhD_Projects/HGNN_Project2/Data_Generation/module1/meta_path.py:82\u001b[0m, in \u001b[0;36mMeta_path.__init__\u001b[0;34m(self, HG, similarity_type, saving_path)\u001b[0m\n\u001b[1;32m     80\u001b[0m W_VDV \u001b[39m=\u001b[39m M(W_vd, W_vd\u001b[39m.\u001b[39mT)\n\u001b[1;32m     81\u001b[0m W_VPV \u001b[39m=\u001b[39m M(W_vp, W_vp\u001b[39m.\u001b[39mT)\n\u001b[0;32m---> 82\u001b[0m W_VLV \u001b[39m=\u001b[39m M(W_vl, W_vl\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m     83\u001b[0m W_VBV \u001b[39m=\u001b[39m M(W_vb, W_vb\u001b[39m.\u001b[39mT)\n\u001b[1;32m     84\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mvisit-visit completed!\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/PhD_Projects/HGNN_Project2/Data_Generation/module1/meta_path.py:309\u001b[0m, in \u001b[0;36mM\u001b[0;34m(W1, W2, n_jobs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39m# Use Parallel to distribute the computation\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmultiplying \u001b[39m\u001b[39m{\u001b[39;00mW1\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m * \u001b[39m\u001b[39m{\u001b[39;00mW2\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m in parallel...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 309\u001b[0m results \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49mn_jobs)(\n\u001b[1;32m    310\u001b[0m     delayed(parallel_multiply_chunk)(W1_csr, W2_csr, row_indices) \u001b[39mfor\u001b[39;49;00m row_indices \u001b[39min\u001b[39;49;00m row_chunks\n\u001b[1;32m    311\u001b[0m )\n\u001b[1;32m    313\u001b[0m \u001b[39m# Stack the results vertically as a sparse matrix\u001b[39;00m\n\u001b[1;32m    314\u001b[0m result \u001b[39m=\u001b[39m vstack(results)\n",
      "File \u001b[0;32m~/anaconda3/envs/envGNN2/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[39m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/envGNN2/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/envGNN2/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   1763\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from module1 import generating_HG as G_class\n",
    "from module1 import XY_preparation as XY\n",
    "from module1 import meta_path as MP\n",
    "\n",
    "# =================================================================================\n",
    "saving_path = f'{disease_data_path}/{num_Diseases}_Diagnoses/{DISEASE_FILE}/{num_Sample}'\n",
    "\n",
    "for p in ['HGNN_data', 'clinical_items', 'GMLs', 'OHV', 'PSGs', 'SNFs']:\n",
    "    os.makedirs(f'{saving_path}/{p}', exist_ok=True)\n",
    "\n",
    "saving_path = f'{disease_data_path}/{num_Diseases}_Diagnoses/{DISEASE_FILE}/{num_Sample}/HGNN_data'\n",
    "# =================================================================================\n",
    "HG_inst = G_class.Generate_HG()\n",
    "\n",
    "G_class.G_statistics(HG_inst.HG)\n",
    "# ======================To sample or not to sample, that is the question =========================\n",
    "if not sampling:\n",
    "    num_Sample = len(HG_inst.Patients)\n",
    "    HG = HG_inst.HG\n",
    "else:\n",
    "    patients_to_remove = random.sample(HG_inst.Patients, len(HG_inst.Patients) - num_Sample)\n",
    "    print(len(patients_to_remove), num_Sample, len(HG_inst.Patients))\n",
    "    \n",
    "    # deleting the nodes\n",
    "    HG = G_class.remove_patients_and_linked_visits(patients_to_remove, HG_inst.HG)\n",
    "\n",
    "# ============================ Extracting Patient-based X and Y =================================\n",
    "XY_inst = XY.XY_preparation(HG)\n",
    "X = XY_inst.X\n",
    "Y = XY_inst.Y\n",
    "# ============================ Extracting Visit-based X and Y =================================\n",
    "XY_inst = XY.XY_preparation(HG)\n",
    "XV = XY_inst.X_visit\n",
    "YV = XY_inst.Y_visit\n",
    "# ======================= Computing the Meta Path based Similarities ======================\n",
    "MP_inst = MP.Meta_path(HG, similarity_type = 'PC', saving_path = saving_path)\n",
    "# ==================================== SAVING =============================================\n",
    "nx.write_gml(HG, f'{saving_path}/HG.gml')\n",
    "save_list_as_pickle(MP_inst.Nodes,   saving_path, 'Nodes')\n",
    "# ==================================== Saving X and Y  (patient-based) ============================\n",
    "torch.save(X, f'{saving_path}/X.pt')\n",
    "torch.save(Y, f'{saving_path}/Y.pt')\n",
    "# ==================================== Saving X and Y (visit-based) =================================\n",
    "torch.save(X, f'{saving_path}/XV.pt')\n",
    "torch.save(Y, f'{saving_path}/YV.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
