{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Define the GNN model\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Aggregation function\n",
    "def aggregate_features(features_list, method='max'):\n",
    "    if method == 'mean':\n",
    "        return torch.mean(torch.stack(features_list), dim=0)\n",
    "    elif method == 'concat':\n",
    "        return torch.cat(features_list, dim=1)\n",
    "    elif method == 'max':\n",
    "        return torch.max(torch.stack(features_list), dim=0).values\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported aggregation method\")\n",
    "\n",
    "# Attention mechanism to compute attention weights\n",
    "class AttentionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_meta_paths):\n",
    "        super(AttentionNetwork, self).__init__()\n",
    "        self.num_meta_paths = num_meta_paths\n",
    "        self.weights = nn.ParameterList([nn.Parameter(torch.randn(input_dim * 2, 1)) for _ in range(num_meta_paths)])\n",
    "    \n",
    "    def forward(self, f_meta):\n",
    "        N = f_meta.shape[0]\n",
    "        attention_weights = torch.zeros(N, N, self.num_meta_paths)\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                combined_features = torch.cat([f_meta[i], f_meta[j]], dim=0)\n",
    "                attention_scores = []\n",
    "                for weight in self.weights:\n",
    "                    score = torch.exp(F.relu(torch.matmul(combined_features, weight)))\n",
    "                    attention_scores.append(score)\n",
    "                attention_scores = torch.stack(attention_scores, dim=-1)\n",
    "                attention_weights[i, j] = attention_scores / torch.sum(attention_scores, dim=-1, keepdim=True)\n",
    "        return attention_weights\n",
    "\n",
    "# HSGNN model\n",
    "class HSGNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_meta_paths, num_classes, aggregation_method='mean'):\n",
    "        super(HSGNN, self).__init__()\n",
    "        self.gnn_layers = nn.ModuleList([GNN(input_dim, output_dim) for _ in range(num_meta_paths)])\n",
    "        self.attention_network = AttentionNetwork(output_dim, num_meta_paths)\n",
    "        self.aggregation_method = aggregation_method\n",
    "        self.final_gnn = GCNConv(output_dim, num_classes)  # Final GCN layer for classification\n",
    "\n",
    "    def forward(self, features, similarity_matrices):\n",
    "        N = features.shape[0]\n",
    "        K = len(similarity_matrices)\n",
    "        \n",
    "        # Generate node features from each similarity matrix using GNNs\n",
    "        features_list = []\n",
    "        for k in range(K):\n",
    "            edge_index = torch.tensor(similarity_matrices[k].nonzero(), dtype=torch.long)\n",
    "            edge_index = torch.stack([edge_index[0], edge_index[1]])  # Ensure correct shape for edge_index\n",
    "            x = features.clone().detach()\n",
    "            features_list.append(self.gnn_layers[k](x, edge_index))\n",
    "        \n",
    "        # Aggregate features to get F_meta\n",
    "        f_meta = aggregate_features(features_list, method=self.aggregation_method)\n",
    "        \n",
    "        # Compute attention weights using F_meta\n",
    "        attention_weights = self.attention_network(f_meta)\n",
    "        \n",
    "        # Compute A_meta\n",
    "        A_meta = torch.zeros(N, N)\n",
    "        for k in range(K):\n",
    "            A_dense = torch.tensor(similarity_matrices[k].toarray(), dtype=torch.float32)  # Convert sparse matrix to dense tensor\n",
    "            A_meta += attention_weights[:, :, k] * A_dense  # Element-wise multiplication\n",
    "        \n",
    "        # Use the aggregated features and integrated adjacency matrix for final prediction\n",
    "        edge_index = torch.tensor(A_meta.nonzero(), dtype=torch.long)\n",
    "        edge_index = torch.stack([edge_index[0], edge_index[1]])  # Ensure correct shape for edge_index\n",
    "        predictions = self.final_gnn(f_meta, edge_index)\n",
    "        \n",
    "        return f_meta, A_meta, predictions\n",
    "\n",
    "# Function to compute micro and macro precision\n",
    "def compute_precisions(predictions, labels):\n",
    "    # Apply sigmoid to get probabilities and then threshold at 0.5\n",
    "    preds = torch.sigmoid(predictions).cpu().detach().numpy()\n",
    "    preds = (preds >= 0.5).astype(int)\n",
    "    labels = labels.cpu().detach().numpy()\n",
    "    \n",
    "    micro_precision = precision_score(labels, preds, average='micro')\n",
    "    macro_precision = precision_score(labels, preds, average='macro')\n",
    "    \n",
    "    return micro_precision, macro_precision\n",
    "\n",
    "# Training and optimization process\n",
    "def train_hsgnn(model, features, similarity_matrices, labels, epochs=100, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss for multi-label classification\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        f_meta, A_meta, predictions = model(features, similarity_matrices)\n",
    "\n",
    "        loss = loss_fn(predictions, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        micro_precision, macro_precision = compute_precisions(predictions, labels)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}, Micro Precision: {micro_precision}, Macro Precision: {macro_precision}\")\n",
    "\n",
    "# =================================================================================================\n",
    "\n",
    "\n",
    "disease_name = 'sample/5000'\n",
    "num_Labels =  100\n",
    "data_path = f'/home/almusawiaf/MyDocuments/PhD_Projects/PSG_SURVIVAL_ANALYSIS/Data/{num_Labels}_Diagnoses/{disease_name}/HGNN_data'\n",
    "\n",
    "print(f'Saving to : {data_path}\\n')\n",
    "\n",
    "features = torch.load(f'{data_path}/X_32.pt')\n",
    "K = 3\n",
    "N = features.shape[0]\n",
    "feature_dim = features.shape[1]\n",
    "output_dim = 16  # Output dimension of GNN\n",
    "\n",
    "print(N, feature_dim)\n",
    "aggregation_method = 'max' # 'mean', 'concat'\n",
    "\n",
    "# Load the similarity matrices\n",
    "similarity_matrices = []\n",
    "for k in range(K):\n",
    "    similarity_matrices.append(scipy.sparse.load_npz(f'{data_path}/A/sparse_matrix_{k}.npz'))\n",
    "    \n",
    "# Ensure features is a PyTorch tensor\n",
    "features = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "labels = torch.load(f'{data_path}/Y.pt')\n",
    "num_classes = labels.shape[1]\n",
    "\n",
    "# =======================================================================================================\n",
    "def synthetic_data():\n",
    "    # Sample data generation\n",
    "    N = 100  # Number of nodes\n",
    "    K = 3  # Number of similarity matrices\n",
    "    feature_dim = 4  # Dimension of node features\n",
    "    output_dim = 16  # Output dimension of GNN\n",
    "    num_classes = 10  # Number of classes for multi-label classification\n",
    "\n",
    "    # Generate random node features\n",
    "    features = torch.randn(N, feature_dim)\n",
    "\n",
    "    # Generate random similarity matrices\n",
    "    similarity_matrices = []\n",
    "    for k in range(K):\n",
    "        matrix = np.random.rand(N, N)\n",
    "        sparse_matrix = scipy.sparse.csr_matrix(matrix)\n",
    "        similarity_matrices.append(sparse_matrix)\n",
    "\n",
    "    # Generate random labels for multi-label classification\n",
    "    labels = torch.randint(0, 2, (N, num_classes)).float()\n",
    "\n",
    "    # Ensure features is a PyTorch tensor\n",
    "    features = torch.tensor(features, dtype=torch.float32)\n",
    "    \n",
    "    return N, K, feature_dim, output_dim, num_classes, features, similarity_matrices, labels\n",
    "# N, K, feature_dim, output_dim, num_classes, features, similarity_matrices, labels = synthetic_data()\n",
    "# =======================================================================================================\n",
    "\n",
    "# Initialize the HSGNN model\n",
    "hs_gnn = HSGNN(feature_dim, output_dim, K, num_classes, aggregation_method='mean')\n",
    "\n",
    "# Train the HSGNN model\n",
    "train_hsgnn(hs_gnn, features, similarity_matrices, labels)\n",
    "\n",
    "# Forward pass to compute F_meta and A_meta\n",
    "hs_gnn.eval()\n",
    "with torch.no_grad():\n",
    "    f_meta, A_meta, predictions = hs_gnn(features, similarity_matrices)\n",
    "\n",
    "print(\"F_meta:\", f_meta)\n",
    "print(f_meta.shape)\n",
    "\n",
    "print(\"A_meta:\", A_meta)\n",
    "print(A_meta.shape)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n",
    "print(predictions.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
