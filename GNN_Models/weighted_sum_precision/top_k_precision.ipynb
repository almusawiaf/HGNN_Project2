{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-2 Precision: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def top_k_precision(y_true, y_scores, k):\n",
    "    \"\"\"\n",
    "    Calculate top-k precision for multilabel classification.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy array): Binary matrix of true labels (shape: n_samples x n_classes).\n",
    "    y_scores (numpy array): Matrix of predicted scores (shape: n_samples x n_classes).\n",
    "    k (int): Number of top elements to consider for calculating precision.\n",
    "\n",
    "    Returns:\n",
    "    float: Mean top-k precision across all samples.\n",
    "    \"\"\"\n",
    "    n_samples = y_true.shape[0]\n",
    "    top_k_precisions = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Get the indices of the top-k predictions\n",
    "        top_k_indices = np.argsort(y_scores[i])[-k:]\n",
    "        \n",
    "        # Calculate precision for this sample\n",
    "        precision = np.sum(y_true[i, top_k_indices]) / k\n",
    "        top_k_precisions.append(precision)\n",
    "    \n",
    "    return np.mean(top_k_precisions)\n",
    "\n",
    "# Example: Ground truth binary matrix\n",
    "y_true = np.array([\n",
    "    [1, 0, 0, 1, 0],\n",
    "    [0, 1, 1, 0, 0],\n",
    "    [1, 1, 0, 0, 0]\n",
    "])\n",
    "\n",
    "# Example: Predicted scores from the model\n",
    "y_scores = np.array([\n",
    "    [0.8, 0.3, 0.2, 0.1, 0.1],\n",
    "    [0.1, 0.7, 0.6, 0.3, 0.2],\n",
    "    [0.9, 0.8, 0.1, 0.4, 0.3]\n",
    "])\n",
    "\n",
    "# Calculate top-2 precision\n",
    "k = 2\n",
    "precision_at_k = top_k_precision(y_true, y_scores, k)\n",
    "print(\"Top-2 Precision:\", precision_at_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8 0.3 0.2 0.1 0.1]\n",
      "[3 4 2 1 0]\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_scores[0])\n",
    "print(np.argsort(y_scores[0]))\n",
    "indices = np.argsort(y_scores[0])[-2:]\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0,\n",
       " 'micro_precision': 0.4,\n",
       " 'macro_precision': 0.39999999999999997,\n",
       " 'recall': 0.8,\n",
       " 'f1_score': 0.52,\n",
       " 'auc': 0,\n",
       " 'top_k_pred': 0.5555555555555555}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, f1_score\n",
    "from sklearn.metrics import classification_report, average_precision_score\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix \n",
    "from sklearn.metrics import accuracy_score, hamming_loss, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch\n",
    "\n",
    "def evaluate(Y, pred_Y, k):\n",
    "    preds = torch.sigmoid(torch.tensor(pred_Y))\n",
    "    binary_preds = (preds > 0.5).float()\n",
    "    \n",
    "    # Ground truth\n",
    "    true_labels = Y\n",
    "    \n",
    "    # Convert to CPU and numpy for sklearn metrics\n",
    "    true_labels_np = true_labels\n",
    "    \n",
    "    binary_preds_np = binary_preds\n",
    "    preds_np = preds\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels_np, binary_preds_np)\n",
    "    \n",
    "    # Calculate Hamming loss\n",
    "    hamming = hamming_loss(true_labels_np, binary_preds_np)\n",
    "    \n",
    "    # Calculate precision, recall, F1 score for micro and macro averaging\n",
    "    precision_micro = precision_score(true_labels_np, binary_preds_np, average='micro', zero_division=0)\n",
    "    recall_micro = recall_score(true_labels_np, binary_preds_np, average='micro', zero_division=0)\n",
    "    f1_micro = f1_score(true_labels_np, binary_preds_np, average='micro', zero_division=0)\n",
    "    \n",
    "    precision_macro = precision_score(true_labels_np, binary_preds_np, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(true_labels_np, binary_preds_np, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(true_labels_np, binary_preds_np, average='macro', zero_division=0)\n",
    "    \n",
    "    # Calculate AUC only if there are both positive and negative samples for each label\n",
    "    try:\n",
    "        auc = roc_auc_score(true_labels_np, preds_np, average='macro', multi_class='ovr') if len(np.unique(true_labels_np)) > 1 else 0\n",
    "    except ValueError:\n",
    "        auc = 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'micro_precision': precision_micro,\n",
    "        'macro_precision': precision_macro,\n",
    "        'recall': recall_macro,\n",
    "        'f1_score': f1_macro,\n",
    "        'auc': auc,\n",
    "        'top_k_pred': top_k_precision(true_labels_np, preds, k),\n",
    "    }\n",
    "\n",
    "evaluate(y_true, y_scores, k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
